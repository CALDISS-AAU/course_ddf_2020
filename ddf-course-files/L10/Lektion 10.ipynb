{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check at de rigtige biblioteker er installeret. Og installer dem, hvis det ikke er tilfældet\n",
    "\n",
    "import sys\n",
    "\n",
    "try: \n",
    "    import requests\n",
    "    print(\"requests library has been imported\")\n",
    "except: \n",
    "    print(\"requests library not found. Installing...\")\n",
    "    !pip install requests\n",
    "    try:\n",
    "        import requests\n",
    "    except: \n",
    "        print(\"Something went wrong in the installation of the requests library. Please check your internet connection and consult output from the installation below\")\n",
    "\n",
    "try:\n",
    "    import pandas\n",
    "    print(\"Pandas library has been imported\")\n",
    "except:\n",
    "    print(\"Pandas library not found. Installing...\")\n",
    "    !pip install pandas\n",
    "    \n",
    "    try:\n",
    "        import pandas\n",
    "    except:\n",
    "        print(\"Something went wrong in the installation of the Pandas library. Please check your internet connection and consult output from the installation below\")\n",
    "\n",
    "try:\n",
    "    import bs4\n",
    "    print(\"BeatifulSoup library has been imported\")\n",
    "except:\n",
    "    print(\"BeatifulSoup library not found. Installing...\")\n",
    "    !pip install beautifulsoup4\n",
    "    \n",
    "    try:\n",
    "        import bs4\n",
    "    except:\n",
    "        print(\"Something went wrong in the installation of the BeatifulSoup library. Please check your internet connection and consult output from the installation below\")\n",
    "\n",
    "try:\n",
    "    import urllib\n",
    "    print(\"Urllib library has been imported\")\n",
    "except:\n",
    "    print(\"Urllib library not found. Installing...\")\n",
    "    !pip install urllib\n",
    "    \n",
    "    try:\n",
    "        import urllib\n",
    "    except:\n",
    "        print(\"Something went wrong in the installation of the Urlllib library. Please check your internet connection and consult output from the installation below\")\n",
    "\n",
    "        \n",
    "try:\n",
    "    import wikipediaapi\n",
    "    print(\"Wikipedia api library has been imported\")\n",
    "except:\n",
    "    print(\"wikipedia api library not found. Installing...\")\n",
    "    !pip install wikipedia-api\n",
    "    \n",
    "    try:\n",
    "        import wikipediaapi\n",
    "    except:\n",
    "        print(\"Something went wrong in the installation of the wikipedia api library. Please check your internet connection and consult output from the installation below\")\n",
    "\n",
    "\n",
    "try: \n",
    "    import networkx\n",
    "    print(\"networkx library has been imported\")\n",
    "except: \n",
    "    print(\"networkx library not found. Installing...\")\n",
    "    !pip install networkx\n",
    "    try:\n",
    "        import networkx\n",
    "    except: \n",
    "        print(\"Something went wrong in the installation of the networkx library. Please check your internet connection and consult output from the installation below\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konstruer et corpus af tekster \n",
    "\n",
    "Før vi kan øve os i forskellige teknikker til tekstanalyse skal vi have et corpus (datasæt) at arbejde på. I nedenstående eksempel genbruger vi scraperen fra lektion 9, der fandt artikler fra infoboxen om machine learning på Wikipedia. Denne gang lader vi scraperen hente selve brødeteksten fra siderne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape tekster fra linksamlingen om 'Machine Learning and Data Science' på Wikipedia\n",
    "import wikipediaapi\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_doc = urllib.request.urlopen(\"https://en.wikipedia.org/wiki/Data_science\")\n",
    "\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "corpus = []\n",
    "\n",
    "\n",
    "pages = []\n",
    "table = soup.find('table', attrs={'class':'vertical-navbox nowraplinks'})\n",
    "table_body = table.find('tbody')\n",
    "\n",
    "for link in table.find_all('a'):\n",
    "    if '/wiki/' in link.get('href'):\n",
    "        pages.append(link.get('href').split('/wiki/')[1])\n",
    "\n",
    "\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "        language='en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Collecting text from \"+str(len(pages))+\" pages...\")\n",
    "\n",
    "for page in pages:\n",
    "    p_wiki = wiki_wiki.page(page)\n",
    "    page_text=p_wiki.text.lower()\n",
    "    corpus.append(page_text)\n",
    "    \n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus ligger nu som en liste af tekster, der kan udforskes enkeltvis\n",
    "corpus[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokeniser teksterne\n",
    "For at kunne analysere corpus bryder vi først tekstdokumenterne ned i enkelte ord (tokens). Det producerer en oversigt over alle de ord, der findes i corpus, hvor mange gange de findes, og hvor mange gange de findes i hvert enkelt dokument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importer de nødvendige biblioteker\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction import stop_words\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kør tokeinsering med almindelig optælling af ordene. \n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words= 'english') # vi bruger en liste med engelske stopord \n",
    "X_count = count_vectorizer.fit_transform(corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# udforsk resultatet\n",
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# udforsk resultatet på dokumentniveau i en Pandas dataframe\n",
    "pd.set_option('display.max_columns', 100)\n",
    "sheet = [count_vectorizer.get_feature_names()]\n",
    "\n",
    "for i in X_count.toarray():\n",
    "    sheet.append(list(i))\n",
    "\n",
    "df_count = pd.DataFrame(sheet)\n",
    "df_count.columns = df_count.iloc[0]\n",
    "df_count = df_count.drop(0)\n",
    "\n",
    "df_count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byg et co-word netværk\n",
    "For at kunne undersøge, hvad det er for et sprog, der karakteriserer de forskellige dokumenter, kan vi producere et co-word netværk. Det er et netværk af ord, der er forbundet til hinanden, når de optræder sammen. For at netværket ikke skal blive for stort sætter vi nogle minimumskriterier på, hvor mange et ord skal optræde i et dokument for at komme i betragtning, og hvor mange dokumenter to ord skal samforekomme i, for at deres forbindelse kan komme med. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# her bruger vi networkx biblioteket til at bygge en netværksfil. \n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "min_occ = 5 #set the minimum occurence count per document\n",
    "\n",
    "edges = {}\n",
    "for x, row in df_count.iterrows():\n",
    "    found_terms = []\n",
    "    for term in df_count.columns:\n",
    "        if row[term] >= min_occ:\n",
    "            found_terms.append(term)\n",
    "    for i,source in enumerate(found_terms):\n",
    "        for target in found_terms[i+1:]:\n",
    "            if source in edges.keys():\n",
    "                if target in edges[source].keys():\n",
    "                    edges[source][target] = edges[source][target] + 1\n",
    "                else: \n",
    "                    edges[source].update({target:1})\n",
    "            elif target in edges.keys():\n",
    "                if source in edges[target].keys():\n",
    "                     edges[target][source] = edges[target][source] + 1\n",
    "                else:\n",
    "                    edges[target].update({source:1})\n",
    "            else:\n",
    "                edges.update({source:{target:1}})\n",
    "                \n",
    "edge_list = []\n",
    "min_edge_weight = 5 # set the minimum co-occurence count for two words to be connected\n",
    "\n",
    "for source in edges:\n",
    "    for target in edges[source]:\n",
    "        if edges[source][target] >= min_edge_weight:\n",
    "            edge = (source,target,{'weight':edges[source][target]})\n",
    "            edge_list.append(edge)\n",
    "\n",
    "G = nx.Graph()   \n",
    "G.add_edges_from(edge_list)\n",
    "\n",
    "nx.write_gexf(G, \"CoWordNET_countvectorizer_minocc\"+str(min_occ)+\"_minedgeweight\"+str(min_edge_weight)+\".gexf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lad os prøve at vægte ordenes væsentlighed med TF-IDF istedet for simpel optælling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kør tokeinsering med TF-IDF \n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words= 'english')\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# udforsk resultatet på dokumentniveau i en Pandas dataframe\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "sheet = [tfidf_vectorizer.get_feature_names()]\n",
    "\n",
    "for i in X_tfidf.toarray():\n",
    "    sheet.append(list(i))\n",
    "\n",
    "df_tfidf = pd.DataFrame(sheet)\n",
    "df_tfidf.columns = df_tfidf.iloc[0]\n",
    "df_tfidf = df_tfidf.drop(0)\n",
    "\n",
    "df_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lad os bygge et co-word netværk igen, men dennegang baseret på TF-IDF vægtning\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "min_occ = 0.03 #set the minimum occurence count\n",
    "\n",
    "edges = {}\n",
    "for x, row in df_tfidf.iterrows():\n",
    "    found_terms = []\n",
    "    for term in df_tfidf.columns:\n",
    "        if row[term] >= min_occ:\n",
    "            found_terms.append(term)\n",
    "    for i,source in enumerate(found_terms):\n",
    "        for target in found_terms[i+1:]:\n",
    "            if source in edges.keys():\n",
    "                if target in edges[source].keys():\n",
    "                    edges[source][target] = edges[source][target] + 1\n",
    "                else: \n",
    "                    edges[source].update({target:1})\n",
    "            elif target in edges.keys():\n",
    "                if source in edges[target].keys():\n",
    "                     edges[target][source] = edges[target][source] + 1\n",
    "                else:\n",
    "                    edges[target].update({source:1})\n",
    "            else:\n",
    "                edges.update({source:{target:1}})\n",
    "edge_list = []\n",
    "min_edge_weight = 5 # set the minimum co-occurence count for two words to be connected\n",
    "\n",
    "for source in edges:\n",
    "    for target in edges[source]:\n",
    "        if edges[source][target] >= min_edge_weight:\n",
    "            edge = (source,target,{'weight':edges[source][target]})\n",
    "            edge_list.append(edge)\n",
    "\n",
    "G = nx.Graph()   \n",
    "G.add_edges_from(edge_list)\n",
    "\n",
    "nx.write_gexf(G, \"CoWordNET_tfidifvectorizer_minocc\"+str(min_occ)+\"_minedgeweight\"+str(min_edge_weight)+\".gexf\")               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sammen netværk, men nu med manuel oprensning af ord\n",
    "\n",
    "cleaning = ['displaystyle']\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "min_occ = 0.03 #set the minimum occurence count\n",
    "\n",
    "edges = {}\n",
    "for x, row in df_tfidf.iterrows():\n",
    "    found_terms = []\n",
    "    for term in df_tfidf.columns:\n",
    "        if term not in cleaning:\n",
    "            if row[term] >= min_occ:\n",
    "                found_terms.append(term)\n",
    "    for i,source in enumerate(found_terms):\n",
    "        for target in found_terms[i+1:]:\n",
    "            if source in edges.keys():\n",
    "                if target in edges[source].keys():\n",
    "                    edges[source][target] = edges[source][target] + 1\n",
    "                else: \n",
    "                    edges[source].update({target:1})\n",
    "            elif target in edges.keys():\n",
    "                if source in edges[target].keys():\n",
    "                     edges[target][source] = edges[target][source] + 1\n",
    "                else:\n",
    "                    edges[target].update({source:1})\n",
    "            else:\n",
    "                edges.update({source:{target:1}})\n",
    "edge_list = []\n",
    "min_edge_weight = 5 # set the minimum co-occurence count for two words to be connected\n",
    "\n",
    "for source in edges:\n",
    "    for target in edges[source]:\n",
    "        if edges[source][target] >= min_edge_weight:\n",
    "            edge = (source,target,{'weight':edges[source][target]})\n",
    "            edge_list.append(edge)\n",
    "\n",
    "G = nx.Graph()   \n",
    "G.add_edges_from(edge_list)\n",
    "\n",
    "nx.write_gexf(G, \"CoWordNET_tfidifvectorizer_minocc\"+str(min_occ)+\"_minedgeweight\"+str(min_edge_weight)+\".gexf\")               \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forsøg med tekstanalyse udover tokenization\n",
    "Hvis vi vil lave mere end basal tokenisering, er vi nødt til at importere et decideret NLP bibliotek. Det giver os mulighed for at lave Part of Speech Tagging, Named Entity Recognition og Sentimentanalyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    import stanza\n",
    "    print(\"stanza library has been imported\")\n",
    "except: \n",
    "    print(\"stanza library not found. Installing...\")\n",
    "    !pip install stanza\n",
    "    try:\n",
    "        import stanza\n",
    "    except: \n",
    "        print(\"Something went wrong in the installation of the networkx library. Please check your internet connection and consult output from the installation below\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "\n",
    "#stanza.download('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos')\n",
    "doc = nlp(corpus[0])\n",
    "print(*[f'word: {word.text}\\tupos: {word.upos}\\txpos: {word.xpos}\\tfeats: {word.feats if word.feats else \"_\"}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')\n",
    "doc = nlp(corpus[0])\n",
    "print(*[f'entity: {ent.text}\\ttype: {ent.type}' for ent in doc.ents], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,sentiment')\n",
    "doc = nlp(corpus[0])\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    if sentence.sentiment == 0:\n",
    "        sentiment = 'NEGATIVE'\n",
    "    if sentence.sentiment == 1:\n",
    "        sentiment = 'NEUTRAL'\n",
    "    if sentence.sentiment == 2:\n",
    "        sentiment = 'POSITIVE'\n",
    "    print(sentiment, sentence.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
