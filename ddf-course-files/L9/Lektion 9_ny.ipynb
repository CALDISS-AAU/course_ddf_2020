{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check at de rigtige biblioteker er installeret. Og installer dem, hvis det ikke er tilfældet\n",
    "\n",
    "import sys\n",
    "\n",
    "try: \n",
    "    import requests\n",
    "    print(\"requests library has been imported\")\n",
    "except: \n",
    "    print(\"requests library not found. Installing...\")\n",
    "    !pip install requests\n",
    "    try:\n",
    "        import requests\n",
    "    except: \n",
    "        print(\"Something went wrong in the installation of the requests library. Please check your internet connection and consult output from the installation below\")\n",
    "\n",
    "try:\n",
    "    import pandas\n",
    "    print(\"Pandas library has been imported\")\n",
    "except:\n",
    "    print(\"Pandas library not found. Installing...\")\n",
    "    !pip install pandas\n",
    "    \n",
    "    try:\n",
    "        import pandas\n",
    "    except:\n",
    "        print(\"Something went wrong in the installation of the Pandas library. Please check your internet connection and consult output from the installation below\")\n",
    "\n",
    "try:\n",
    "    import bs4\n",
    "    print(\"BeatifulSoup library has been imported\")\n",
    "except:\n",
    "    print(\"BeatifulSoup library not found. Installing...\")\n",
    "    !pip install beautifulsoup4\n",
    "    \n",
    "    try:\n",
    "        import bs4\n",
    "    except:\n",
    "        print(\"Something went wrong in the installation of the BeatifulSoup library. Please check your internet connection and consult output from the installation below\")\n",
    "\n",
    "try:\n",
    "    import urllib\n",
    "    print(\"Urllib library has been imported\")\n",
    "except:\n",
    "    print(\"Urllib library not found. Installing...\")\n",
    "    !pip install urllib\n",
    "    \n",
    "    try:\n",
    "        import urllib\n",
    "    except:\n",
    "        print(\"Something went wrong in the installation of the Urlllib library. Please check your internet connection and consult output from the installation below\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lad os prøve at scrape noget data direkte fra Wikipedia\n",
    "Din browser viser indholdet af en hjemmeside baseret på nogle instrukser i HTML-koden. Faktisk kunne man sige, at HTML et semi-struktureret dataformat, der associerer tekst med metadata. For eksempel fortæller HTML-koden din browser hvilke dele af teksten der er brødtekst, hvilke dele der er overskrifter, hvilke dele, der skal rammes ind af en tekstboks, og så videre. \n",
    "\n",
    "En scraper udnytter den metadata, der ligger i HTML-koden, til at udvælge og hente betstemte informationer. Vi kan starte med at prøve at hente den fulde HTML-kode fra en af siderne på Wikipedia for at få en idé om, hvordan det ser ud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hent HTML-koden fra artiklen om Data Science på Wikipedia \n",
    "\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_doc = urllib.request.urlopen(\"https://en.wikipedia.org/wiki/Data_science\")\n",
    "\n",
    "soup = BeautifulSoup(html_doc)\n",
    "print(soup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vi kunne for eksempel scrape alle links fra en side\n",
    "Hvis vi er interesserede i alle hyperlinks vi finder på siden, kan vi bruge HTML-tagget 'href' (hyperlink reference) til udvælge de relevante informationer fra den rå HTML-kode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find de dele af teksten, der er markeret med 'href' (links)\n",
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Det kunne også være, vi var interesserede i links fra et bestemt sted på siden\n",
    "Ved at finde de relvante HTML-tags for den linksamling om 'Machine Learning and Data Sciene' der findes øverst til højre på artikle om Data Science kan vi indsnævre scraperens fokus til kun at hente links herfra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find linksamlingen om 'Machine Learning and Data Science' og høst links herfra\n",
    "\n",
    "table = soup.find('table', attrs={'class':'vertical-navbox nowraplinks'})\n",
    "table_body = table.find('tbody')\n",
    "\n",
    "for link in table.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En crawler kan gøre os i stand til at scrape flere sider\n",
    "Hvis vi vil scrape links fra en masse forskellige sider på Wikipedia, har vi to muligheder. Enten har vi en liste, som vi i forvejen ved er interessant, eller også skal vores scraper være i stand til selv at bevæge sig videre til nye sider, når den finder nogle relevante informationer på den side, den er igang med at høste data fra. Den sidste løsning kaldes 'crawling'. \n",
    "\n",
    "Vi kunne for eksmpel bede vores scraper om at starte på artiklen om 'Data Science', lokalisere linksamlingen om 'Machine Learning and Data Science', og så besøge (crawle) alle de sider, der linkes til herfra. Når crawleren besøger de nye sider kan den hente alle de links, den finder hvert sted. \n",
    "\n",
    "I princippet kunne vi crawleren fortsætte i uendelighed indtil alle sider på Wikipedia var scrapet - eller alle sider på hele internettet, for den sags skyld. I praksis vil man ofte bestemme, hvor lang snor cralweren for. Dette kaldes 'crawl distance'. I dette tilfælde er crawl distance = 1, fordi vi giver den lov til at tage 1 hyperlink skridt væk fra udgangspunktet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crawl de sider, der linkes til i linksamlingen om 'Machine Learning and Data Science' og scrape alle deres links\n",
    "#Gem resultatet som JSON\n",
    "\n",
    "data = {}\n",
    "\n",
    "for link in table.find_all('a'):\n",
    "    if link.get('href').startswith('/wiki/'):\n",
    "        html_doc = urllib.request.urlopen('https://en.wikipedia.org'+link.get('href'))\n",
    "        soup = BeautifulSoup(html_doc)\n",
    "        data.update({link.get('href'):[]})\n",
    "        for new_link in soup.find_all('a'):\n",
    "            if type(new_link.get('href')) is str:\n",
    "                if new_link.get('href').startswith('/wiki/'):\n",
    "                    data[link.get('href')].append(new_link.get('href'))\n",
    "\n",
    "with open('CrawledWikitable_Links.json', 'w',encoding='utf8') as f:\n",
    "    json.dump(data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#få overblik over hvilke sider, crawleren har besøgt\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#udforsk links fra en bestemt side\n",
    "data['/wiki/Computational_learning_theory']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hent data der ikke er umiddelbart tilgængelig i HTML-koden via Wikipedias API \n",
    "De fleste medier har et Application Programming Interface (API). Det er en portal, der gør det muligt for en applikation at tilgå mediets database automatisk. For at kunne skrive et program, der kan 'ringe op' til en API og bede om bestemte oplysninger, er man nødt til at kende syntaxen. Derudover bestemmer ejeren af APIen hvilke informationer, der er tilgængelige, og på hvilke betingelser. De fleste af API'er har en dokumentation, hvor man kan læse om reglerne.\n",
    "\n",
    "Vi kunne bede Wikipedias API om informationer, der ikke ummidelbart er tilgængelig for vores scraper i en artikels HTML kode. Det kunne for eksempel handle om, hvordan siden er blevet redigeret. Det er muligt at finde disse informationer andetsteds, men det er praktisk at vi bare kan bede om dem gennem APIen. Dokumentationen findes her: https://www.mediawiki.org/wiki/API:Revisions#API_documentation\n",
    "\n",
    "Herunder ringer vi op til APIen på adressen http://en.wikipedia.org/w/api.php og beder om informationer ved at bruge nogle af de parametre, der er beskrevet i dokumentationen (også kaldet endpoints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kald Wikipedias API for at få revisioner på artiklen om 'Data Science'\n",
    "import requests\n",
    "\n",
    "S = requests.Session()\n",
    "\n",
    "URL = \"http://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"titles\": \"Data science\",\n",
    "        \"rvlimit\": \"500\",\n",
    "        \"rvprop\": \"timestamp\",\n",
    "        \"rvdir\": \"newer\",\n",
    "        \"formatversion\": \"2\",\n",
    "        \"format\": \"json\"\n",
    "\n",
    "    }\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "DATA = R.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#udforsk resultatet\n",
    "DATA['query']['pages'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Udvid API-kaldet med følgende informationer (listet under 'rvprop' parametret):\n",
    "#hvilke brugere har lavet hver enkelt revision\n",
    "#hvor store er revisionerne\n",
    "#hvilke kommentaerer har brugerne knyttet til revisionerne\n",
    "\n",
    "import requests\n",
    "\n",
    "S = requests.Session()\n",
    "\n",
    "URL = \"http://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"titles\": \"Data science\",\n",
    "        \"rvlimit\": \"500\",\n",
    "        \"rvprop\": \"user|timestamp|size|comment\",\n",
    "        \"rvdir\": \"newer\",\n",
    "        \"formatversion\": \"2\",\n",
    "        \"format\": \"json\"\n",
    "\n",
    "    }\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "DATA = R.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#udforsk resultatet\n",
    "DATA['query']['pages'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse engineering øvelse\n",
    "Prøv at åbne det værktøj, der hedder SeeAlsology: https://densitydesign.github.io/strumentalia-seealsology/\n",
    "SeeAlsology bruger en blanding af scraping, crawling og API-kald til bygge et dataset af Wikipedia artikler op omkring de links, den finder i den såkaldte 'See Also' sektion i bunden af en side. Du kan for eksempel prøve at bruge artiklen om Big Data, der har en fyldig 'See Also' sektion, som input: https://en.wikipedia.org/wiki/Big_data\n",
    "\n",
    "KAN DU FORKLARE HVORDAN VÆRKTØJET VIRKER?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
